{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approaching Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>bin_0</th>\n",
       "      <th>bin_1</th>\n",
       "      <th>bin_2</th>\n",
       "      <th>bin_3</th>\n",
       "      <th>bin_4</th>\n",
       "      <th>nom_0</th>\n",
       "      <th>nom_1</th>\n",
       "      <th>nom_2</th>\n",
       "      <th>nom_3</th>\n",
       "      <th>...</th>\n",
       "      <th>nom_9</th>\n",
       "      <th>ord_0</th>\n",
       "      <th>ord_1</th>\n",
       "      <th>ord_2</th>\n",
       "      <th>ord_3</th>\n",
       "      <th>ord_4</th>\n",
       "      <th>ord_5</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>42861</th>\n",
       "      <td>42861</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Circle</td>\n",
       "      <td>Hamster</td>\n",
       "      <td>Costa Rica</td>\n",
       "      <td>...</td>\n",
       "      <td>db98264ef</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Novice</td>\n",
       "      <td>NaN</td>\n",
       "      <td>e</td>\n",
       "      <td>X</td>\n",
       "      <td>MV</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258103</th>\n",
       "      <td>258103</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>T</td>\n",
       "      <td>Y</td>\n",
       "      <td>Red</td>\n",
       "      <td>Trapezoid</td>\n",
       "      <td>Axolotl</td>\n",
       "      <td>Finland</td>\n",
       "      <td>...</td>\n",
       "      <td>0d5573e59</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Novice</td>\n",
       "      <td>Boiling Hot</td>\n",
       "      <td>e</td>\n",
       "      <td>U</td>\n",
       "      <td>th</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592454</th>\n",
       "      <td>592454</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "      <td>Red</td>\n",
       "      <td>Circle</td>\n",
       "      <td>Lion</td>\n",
       "      <td>India</td>\n",
       "      <td>...</td>\n",
       "      <td>004b4670d</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Expert</td>\n",
       "      <td>Warm</td>\n",
       "      <td>k</td>\n",
       "      <td>Q</td>\n",
       "      <td>oh</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>539892</th>\n",
       "      <td>539892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Square</td>\n",
       "      <td>Lion</td>\n",
       "      <td>Russia</td>\n",
       "      <td>...</td>\n",
       "      <td>0d5573e59</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cold</td>\n",
       "      <td>j</td>\n",
       "      <td>H</td>\n",
       "      <td>WW</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313607</th>\n",
       "      <td>313607</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Red</td>\n",
       "      <td>Polygon</td>\n",
       "      <td>Hamster</td>\n",
       "      <td>Costa Rica</td>\n",
       "      <td>...</td>\n",
       "      <td>a8ac5fac2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Contributor</td>\n",
       "      <td>Lava Hot</td>\n",
       "      <td>f</td>\n",
       "      <td>Y</td>\n",
       "      <td>DT</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114748</th>\n",
       "      <td>114748</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "      <td>Red</td>\n",
       "      <td>Circle</td>\n",
       "      <td>Axolotl</td>\n",
       "      <td>India</td>\n",
       "      <td>...</td>\n",
       "      <td>9fbf93590</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Master</td>\n",
       "      <td>Warm</td>\n",
       "      <td>c</td>\n",
       "      <td>T</td>\n",
       "      <td>oh</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467565</th>\n",
       "      <td>467565</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Trapezoid</td>\n",
       "      <td>Lion</td>\n",
       "      <td>Costa Rica</td>\n",
       "      <td>...</td>\n",
       "      <td>836203022</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Grandmaster</td>\n",
       "      <td>Freezing</td>\n",
       "      <td>e</td>\n",
       "      <td>Y</td>\n",
       "      <td>fF</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555169</th>\n",
       "      <td>555169</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Red</td>\n",
       "      <td>Trapezoid</td>\n",
       "      <td>Hamster</td>\n",
       "      <td>Finland</td>\n",
       "      <td>...</td>\n",
       "      <td>266a69792</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Expert</td>\n",
       "      <td>Cold</td>\n",
       "      <td>a</td>\n",
       "      <td>K</td>\n",
       "      <td>RT</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272394</th>\n",
       "      <td>272394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Circle</td>\n",
       "      <td>Axolotl</td>\n",
       "      <td>Costa Rica</td>\n",
       "      <td>...</td>\n",
       "      <td>b29c5465a</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Novice</td>\n",
       "      <td>Freezing</td>\n",
       "      <td>n</td>\n",
       "      <td>P</td>\n",
       "      <td>Hk</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397322</th>\n",
       "      <td>397322</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Red</td>\n",
       "      <td>Triangle</td>\n",
       "      <td>Axolotl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>ea8b5bac0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Contributor</td>\n",
       "      <td>Cold</td>\n",
       "      <td>k</td>\n",
       "      <td>C</td>\n",
       "      <td>JT</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  bin_0  bin_1  bin_2 bin_3 bin_4 nom_0      nom_1    nom_2  \\\n",
       "42861    42861    0.0    1.0    0.0     F     N  Blue     Circle  Hamster   \n",
       "258103  258103    0.0    0.0    0.0     T     Y   Red  Trapezoid  Axolotl   \n",
       "592454  592454    0.0    0.0    0.0     F     Y   Red     Circle     Lion   \n",
       "539892  539892    0.0    0.0    0.0     F     Y  Blue     Square     Lion   \n",
       "313607  313607    0.0    0.0    1.0     F     N   Red    Polygon  Hamster   \n",
       "114748  114748    0.0    0.0    0.0     F     Y   Red     Circle  Axolotl   \n",
       "467565  467565    0.0    0.0    0.0     F     Y  Blue  Trapezoid     Lion   \n",
       "555169  555169    0.0    0.0    0.0     F     N   Red  Trapezoid  Hamster   \n",
       "272394  272394    0.0    0.0    0.0     F     Y  Blue     Circle  Axolotl   \n",
       "397322  397322    0.0    0.0    0.0     F   NaN   Red   Triangle  Axolotl   \n",
       "\n",
       "             nom_3  ...      nom_9 ord_0        ord_1        ord_2 ord_3  \\\n",
       "42861   Costa Rica  ...  db98264ef   1.0       Novice          NaN     e   \n",
       "258103     Finland  ...  0d5573e59   1.0       Novice  Boiling Hot     e   \n",
       "592454       India  ...  004b4670d   2.0       Expert         Warm     k   \n",
       "539892      Russia  ...  0d5573e59   3.0          NaN         Cold     j   \n",
       "313607  Costa Rica  ...  a8ac5fac2   2.0  Contributor     Lava Hot     f   \n",
       "114748       India  ...  9fbf93590   2.0       Master         Warm     c   \n",
       "467565  Costa Rica  ...  836203022   1.0  Grandmaster     Freezing     e   \n",
       "555169     Finland  ...  266a69792   1.0       Expert         Cold     a   \n",
       "272394  Costa Rica  ...  b29c5465a   3.0       Novice     Freezing     n   \n",
       "397322         NaN  ...  ea8b5bac0   1.0  Contributor         Cold     k   \n",
       "\n",
       "       ord_4  ord_5  day month target  \n",
       "42861      X     MV  6.0   2.0      0  \n",
       "258103     U     th  4.0   3.0      0  \n",
       "592454     Q     oh  5.0   1.0      0  \n",
       "539892     H     WW  5.0   8.0      1  \n",
       "313607     Y     DT  1.0   1.0      1  \n",
       "114748     T     oh  NaN   3.0      0  \n",
       "467565     Y     fF  7.0   7.0      0  \n",
       "555169     K     RT  5.0   7.0      0  \n",
       "272394     P     Hk  3.0   5.0      0  \n",
       "397322     C     JT  1.0   4.0      0  \n",
       "\n",
       "[10 rows x 25 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv(\"cat_train.csv\")\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Hot', 'Warm', 'Freezing', 'Lava Hot', 'Cold', 'Boiling Hot', nan],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View unique values in column ord_2\n",
    "df['ord_2'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Freezing       142726\n",
       "Warm           124239\n",
       "Cold            97822\n",
       "Boiling Hot     84790\n",
       "Hot             67508\n",
       "Lava Hot        64840\n",
       "Name: ord_2, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the value counts before mapping\n",
    "df.ord_2.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most important ways to handle categorical variables\n",
    "1. Label Encoding\n",
    "2. Sparse Matrices\n",
    "3. One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    \"Freezing\": 0,\n",
    "    \"Warm\": 1,\n",
    "    \"Cold\": 2,\n",
    "    \"Boiling Hot\": 3,\n",
    "    \"Hot\": 4,\n",
    "    \"Lava Hot\": 5\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    142726\n",
       "1.0    124239\n",
       "2.0     97822\n",
       "3.0     84790\n",
       "4.0     67508\n",
       "5.0     64840\n",
       "Name: ord_2, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label Encoding - Convert categories to numbers\n",
    "df.loc[:, \"ord_2\"] = df.ord_2.map(mapping)\n",
    "\n",
    "# View the value counts after mapping\n",
    "df.ord_2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>bin_0</th>\n",
       "      <th>bin_1</th>\n",
       "      <th>bin_2</th>\n",
       "      <th>bin_3</th>\n",
       "      <th>bin_4</th>\n",
       "      <th>nom_0</th>\n",
       "      <th>nom_1</th>\n",
       "      <th>nom_2</th>\n",
       "      <th>nom_3</th>\n",
       "      <th>...</th>\n",
       "      <th>nom_9</th>\n",
       "      <th>ord_0</th>\n",
       "      <th>ord_1</th>\n",
       "      <th>ord_2</th>\n",
       "      <th>ord_3</th>\n",
       "      <th>ord_4</th>\n",
       "      <th>ord_5</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Red</td>\n",
       "      <td>Trapezoid</td>\n",
       "      <td>Hamster</td>\n",
       "      <td>Russia</td>\n",
       "      <td>...</td>\n",
       "      <td>02e7c8990</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Contributor</td>\n",
       "      <td>3</td>\n",
       "      <td>c</td>\n",
       "      <td>U</td>\n",
       "      <td>Pw</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "      <td>Red</td>\n",
       "      <td>Star</td>\n",
       "      <td>Axolotl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>f37df64af</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Grandmaster</td>\n",
       "      <td>6</td>\n",
       "      <td>e</td>\n",
       "      <td>X</td>\n",
       "      <td>pE</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Red</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hamster</td>\n",
       "      <td>Canada</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>n</td>\n",
       "      <td>P</td>\n",
       "      <td>eN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Red</td>\n",
       "      <td>Circle</td>\n",
       "      <td>Hamster</td>\n",
       "      <td>Finland</td>\n",
       "      <td>...</td>\n",
       "      <td>f9d456e57</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Novice</td>\n",
       "      <td>4</td>\n",
       "      <td>a</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>Red</td>\n",
       "      <td>Triangle</td>\n",
       "      <td>Hamster</td>\n",
       "      <td>Costa Rica</td>\n",
       "      <td>...</td>\n",
       "      <td>c5361037c</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Grandmaster</td>\n",
       "      <td>1</td>\n",
       "      <td>h</td>\n",
       "      <td>C</td>\n",
       "      <td>OZ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  bin_0  bin_1  bin_2 bin_3 bin_4 nom_0      nom_1    nom_2       nom_3  \\\n",
       "0   0    0.0    0.0    0.0     F     N   Red  Trapezoid  Hamster      Russia   \n",
       "1   1    1.0    1.0    0.0     F     Y   Red       Star  Axolotl         NaN   \n",
       "2   2    0.0    1.0    0.0     F     N   Red        NaN  Hamster      Canada   \n",
       "3   3    NaN    0.0    0.0     F     N   Red     Circle  Hamster     Finland   \n",
       "4   4    0.0    NaN    0.0     T     N   Red   Triangle  Hamster  Costa Rica   \n",
       "\n",
       "   ...      nom_9 ord_0        ord_1 ord_2 ord_3 ord_4  ord_5  day  month  \\\n",
       "0  ...  02e7c8990   3.0  Contributor     3     c     U     Pw  6.0    3.0   \n",
       "1  ...  f37df64af   3.0  Grandmaster     6     e     X     pE  7.0    7.0   \n",
       "2  ...        NaN   3.0          NaN     2     n     P     eN  5.0    9.0   \n",
       "3  ...  f9d456e57   1.0       Novice     4     a     C    NaN  3.0    3.0   \n",
       "4  ...  c5361037c   3.0  Grandmaster     1     h     C     OZ  5.0   12.0   \n",
       "\n",
       "  target  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label Encoding using scikit-learn\n",
    "import pandas as pd \n",
    "from sklearn import preprocessing\n",
    "\n",
    "# read the data\n",
    "df = pd.read_csv(\"cat_train.csv\")\n",
    "\n",
    "# fill the NaN values in ord_2 column (because LabelEncoder of scikit-learn does not handle NaN values)\n",
    "df.loc[:, \"ord_2\"] = df.ord_2.fillna(\"NONE\")\n",
    "\n",
    "# initialize LabelEncoder\n",
    "lbl_enc = preprocessing.LabelEncoder()\n",
    "\n",
    "# fit label encoder and transform values on ord_2 column\n",
    "# P.S.: do not use this directly, fit first and then transform\n",
    "df.loc[:, \"ord_2\"] = lbl_enc.fit_transform(df.ord_2.values)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    }
   ],
   "source": [
    "# size of a numpy array\n",
    "import numpy as np\n",
    "\n",
    "# create our example feature matrix\n",
    "example = np.array(\n",
    "    [\n",
    "        [0, 0, 1],\n",
    "        [1, 0, 0],\n",
    "        [1, 0, 1]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# print size in bytes\n",
    "print(example.nbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "# convert numpy array to sparse matrix\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "# create our example feature matrix\n",
    "example = np.array(\n",
    "    [\n",
    "        [0, 0, 1],\n",
    "        [1, 0, 0],\n",
    "        [1, 0, 1]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# convert numpy array to sparse CSR matrix\n",
    "sparse_example = sparse.csr_matrix(example)\n",
    "\n",
    "# print size of this sparse matrix\n",
    "print(sparse_example.data.nbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "# total size of sparse csr matrix\n",
    "print(\n",
    "    sparse_example.data.nbytes +\n",
    "    sparse_example.indptr.nbytes + \n",
    "    sparse_example.indices.nbytes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dense array: 4000000000\n",
      "Size of sparse array: 200000148\n",
      "Full size of sparse array: 400040300\n"
     ]
    }
   ],
   "source": [
    "# Comparison of sizes of dense and sparse arrays\n",
    "import numpy as np \n",
    "from scipy import sparse \n",
    "\n",
    "# number of rows\n",
    "n_rows = 10000\n",
    "\n",
    "# number of columns\n",
    "n_cols = 100000\n",
    "\n",
    "# create random binary matrix with only 5% values as 1s\n",
    "example = np.random.binomial(1, p=0.05, size=(n_rows, n_cols))\n",
    "\n",
    "# print size in bytes\n",
    "print(f\"Size of dense array: {example.nbytes}\")\n",
    "\n",
    "# convert numpy array to sparse CSR matrix\n",
    "sparse_example = sparse.csr_matrix(example)\n",
    "\n",
    "# print size of this sparse matrix\n",
    "print(f\"Size of sparse array: {sparse_example.data.nbytes}\")\n",
    "\n",
    "full_size = (\n",
    "    sparse_example.data.nbytes +\n",
    "    sparse_example.indptr.nbytes + \n",
    "    sparse_example.indices.nbytes\n",
    ")\n",
    "\n",
    "# print full size of this sparse matrix\n",
    "print(f\"Full size of sparse array: {full_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dense array: 72\n",
      "Size of sparse array: 12\n",
      "Full size of sparse array: 40\n"
     ]
    }
   ],
   "source": [
    "# Comparison of sizes of dense and binarized arrays\n",
    "import numpy as np \n",
    "from scipy import sparse \n",
    "\n",
    "# create binary matrix\n",
    "example = np.array(\n",
    "    [\n",
    "        [0, 0, 0, 0, 1, 0],\n",
    "        [0, 1, 0, 0, 0, 0],\n",
    "        [1, 0, 0, 0, 0, 0]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# print size in bytes\n",
    "print(f\"Size of dense array: {example.nbytes}\")\n",
    "\n",
    "# convert numpy array to sparse CSR matrix\n",
    "sparse_example = sparse.csr_matrix(example)\n",
    "\n",
    "# print size of this sparse matrix\n",
    "print(f\"Size of sparse array: {sparse_example.data.nbytes}\")\n",
    "\n",
    "full_size = (\n",
    "    sparse_example.data.nbytes +\n",
    "    sparse_example.indptr.nbytes + \n",
    "    sparse_example.indices.nbytes\n",
    ")\n",
    "\n",
    "# print full size of this sparse matrix\n",
    "print(f\"Full size of sparse array: {full_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of dense array: 8000000000\n",
      "Size of sparse array: 8000000\n",
      "Full size of sparse array: 16000004\n"
     ]
    }
   ],
   "source": [
    "# Use scikit-learn's OneHotEncoder to transform a much larger feature array with 1001 categories\n",
    "import numpy as np \n",
    "from sklearn import preprocessing\n",
    "\n",
    "# create random 1D array with 1001 different categories (int)\n",
    "example = np.random.randint(1000, size=1000000)\n",
    "\n",
    "# initialize OneHotEncoder from scikit-learn\n",
    "# keep sparse = False to get dense array\n",
    "ohe = preprocessing.OneHotEncoder(sparse=False)\n",
    "\n",
    "# fit and transform data with dense one hot encoder\n",
    "ohe_example = ohe.fit_transform(example.reshape(-1, 1))\n",
    "\n",
    "# print size in bytes for dense array\n",
    "print(f\"Size of dense array: {ohe_example.nbytes}\")\n",
    "\n",
    "# initialize OneHotEncoder from scikit-learn\n",
    "# keep sparse = True to get sparse array\n",
    "ohe = preprocessing.OneHotEncoder(sparse=True)\n",
    "\n",
    "# fit and transform data with sparse one hot encoder\n",
    "ohe_example = ohe.fit_transform(example.reshape(-1, 1))\n",
    "\n",
    "# print size in bytes for sparse array\n",
    "print(f\"Size of sparse array: {ohe_example.data.nbytes}\")\n",
    "\n",
    "full_size = (\n",
    "    ohe_example.data.nbytes +\n",
    "    ohe_example.indptr.nbytes +\n",
    "    ohe_example.indices.nbytes\n",
    ")\n",
    "\n",
    "# print full size of this sparse matrix\n",
    "print(f\"Full size of sparse array: {full_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other ways to handle categorical variables:\n",
    "\n",
    "#### - Converting categorical variables to numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84790, 25)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of dataframe where 'ord_2' column has value 'Boiling Hot'\n",
    "import pandas as pd \n",
    "df = pd.read_csv(\"cat_train.csv\")\n",
    "df[df.ord_2 == 'Boiling Hot'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ord_2\n",
       "Boiling Hot     84790\n",
       "Cold            97822\n",
       "Freezing       142726\n",
       "Hot             67508\n",
       "Lava Hot        64840\n",
       "Warm           124239\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate above with pandas groupby function\n",
    "df.groupby(\"ord_2\")[\"id\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          67508.0\n",
       "1         124239.0\n",
       "2         142726.0\n",
       "3          64840.0\n",
       "4          97822.0\n",
       "            ...   \n",
       "599995    142726.0\n",
       "599996     84790.0\n",
       "599997    142726.0\n",
       "599998    124239.0\n",
       "599999     84790.0\n",
       "Name: id, Length: 600000, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace ord_2 column with its count values to convert it to numerical column (using transform function of pandas)\n",
    "df.groupby(\"ord_2\")[\"id\"].transform(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ord_1</th>\n",
       "      <th>ord_2</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Contributor</td>\n",
       "      <td>Boiling Hot</td>\n",
       "      <td>15634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Contributor</td>\n",
       "      <td>Cold</td>\n",
       "      <td>17734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Contributor</td>\n",
       "      <td>Freezing</td>\n",
       "      <td>26082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Contributor</td>\n",
       "      <td>Hot</td>\n",
       "      <td>12428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Contributor</td>\n",
       "      <td>Lava Hot</td>\n",
       "      <td>11919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Contributor</td>\n",
       "      <td>Warm</td>\n",
       "      <td>22774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Expert</td>\n",
       "      <td>Boiling Hot</td>\n",
       "      <td>19477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Expert</td>\n",
       "      <td>Cold</td>\n",
       "      <td>22956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Expert</td>\n",
       "      <td>Freezing</td>\n",
       "      <td>33249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Expert</td>\n",
       "      <td>Hot</td>\n",
       "      <td>15792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Expert</td>\n",
       "      <td>Lava Hot</td>\n",
       "      <td>15078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Expert</td>\n",
       "      <td>Warm</td>\n",
       "      <td>28900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Grandmaster</td>\n",
       "      <td>Boiling Hot</td>\n",
       "      <td>13623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Grandmaster</td>\n",
       "      <td>Cold</td>\n",
       "      <td>15464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Grandmaster</td>\n",
       "      <td>Freezing</td>\n",
       "      <td>22818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Grandmaster</td>\n",
       "      <td>Hot</td>\n",
       "      <td>10805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Grandmaster</td>\n",
       "      <td>Lava Hot</td>\n",
       "      <td>10363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Grandmaster</td>\n",
       "      <td>Warm</td>\n",
       "      <td>19899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Master</td>\n",
       "      <td>Boiling Hot</td>\n",
       "      <td>10800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Master</td>\n",
       "      <td>Cold</td>\n",
       "      <td>12364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Master</td>\n",
       "      <td>Freezing</td>\n",
       "      <td>18035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Master</td>\n",
       "      <td>Hot</td>\n",
       "      <td>8594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Master</td>\n",
       "      <td>Lava Hot</td>\n",
       "      <td>8209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Master</td>\n",
       "      <td>Warm</td>\n",
       "      <td>15734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Novice</td>\n",
       "      <td>Boiling Hot</td>\n",
       "      <td>22718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Novice</td>\n",
       "      <td>Cold</td>\n",
       "      <td>26271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Novice</td>\n",
       "      <td>Freezing</td>\n",
       "      <td>38233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Novice</td>\n",
       "      <td>Hot</td>\n",
       "      <td>17850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Novice</td>\n",
       "      <td>Lava Hot</td>\n",
       "      <td>17373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Novice</td>\n",
       "      <td>Warm</td>\n",
       "      <td>33263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ord_1        ord_2  count\n",
       "0   Contributor  Boiling Hot  15634\n",
       "1   Contributor         Cold  17734\n",
       "2   Contributor     Freezing  26082\n",
       "3   Contributor          Hot  12428\n",
       "4   Contributor     Lava Hot  11919\n",
       "5   Contributor         Warm  22774\n",
       "6        Expert  Boiling Hot  19477\n",
       "7        Expert         Cold  22956\n",
       "8        Expert     Freezing  33249\n",
       "9        Expert          Hot  15792\n",
       "10       Expert     Lava Hot  15078\n",
       "11       Expert         Warm  28900\n",
       "12  Grandmaster  Boiling Hot  13623\n",
       "13  Grandmaster         Cold  15464\n",
       "14  Grandmaster     Freezing  22818\n",
       "15  Grandmaster          Hot  10805\n",
       "16  Grandmaster     Lava Hot  10363\n",
       "17  Grandmaster         Warm  19899\n",
       "18       Master  Boiling Hot  10800\n",
       "19       Master         Cold  12364\n",
       "20       Master     Freezing  18035\n",
       "21       Master          Hot   8594\n",
       "22       Master     Lava Hot   8209\n",
       "23       Master         Warm  15734\n",
       "24       Novice  Boiling Hot  22718\n",
       "25       Novice         Cold  26271\n",
       "26       Novice     Freezing  38233\n",
       "27       Novice          Hot  17850\n",
       "28       Novice     Lava Hot  17373\n",
       "29       Novice         Warm  33263"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can also group by multiple columns and their counts\n",
    "df.groupby(\n",
    "    [\n",
    "        \"ord_1\",\n",
    "        \"ord_2\"\n",
    "    ]\n",
    ")[\"id\"].count().reset_index(name=\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Create new categorical features from the existing ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                 Contributor_Hot\n",
       "1                Grandmaster_Warm\n",
       "2                    nan_Freezing\n",
       "3                 Novice_Lava Hot\n",
       "4                Grandmaster_Cold\n",
       "                   ...           \n",
       "599995            Novice_Freezing\n",
       "599996         Novice_Boiling Hot\n",
       "599997       Contributor_Freezing\n",
       "599998                Master_Warm\n",
       "599999    Contributor_Boiling Hot\n",
       "Name: new_feature, Length: 600000, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenating names of ord_1 and ord_2 columns using an underscore\n",
    "df[\"new_feature\"] = (\n",
    "    df.ord_1.astype(str)\n",
    "    + \"_\"\n",
    "    + df.ord_2.astype(str)\n",
    ")\n",
    "df.new_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>bin_0</th>\n",
       "      <th>bin_1</th>\n",
       "      <th>bin_2</th>\n",
       "      <th>bin_3</th>\n",
       "      <th>bin_4</th>\n",
       "      <th>nom_0</th>\n",
       "      <th>nom_1</th>\n",
       "      <th>nom_2</th>\n",
       "      <th>nom_3</th>\n",
       "      <th>...</th>\n",
       "      <th>ord_0</th>\n",
       "      <th>ord_1</th>\n",
       "      <th>ord_2</th>\n",
       "      <th>ord_3</th>\n",
       "      <th>ord_4</th>\n",
       "      <th>ord_5</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>target</th>\n",
       "      <th>new_feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Red</td>\n",
       "      <td>Trapezoid</td>\n",
       "      <td>Hamster</td>\n",
       "      <td>Russia</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Contributor</td>\n",
       "      <td>Hot</td>\n",
       "      <td>c</td>\n",
       "      <td>U</td>\n",
       "      <td>Pw</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Contributor_Hot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "      <td>Red</td>\n",
       "      <td>Star</td>\n",
       "      <td>Axolotl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Grandmaster</td>\n",
       "      <td>Warm</td>\n",
       "      <td>e</td>\n",
       "      <td>X</td>\n",
       "      <td>pE</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Grandmaster_Warm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Red</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hamster</td>\n",
       "      <td>Canada</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Freezing</td>\n",
       "      <td>n</td>\n",
       "      <td>P</td>\n",
       "      <td>eN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>nan_Freezing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Red</td>\n",
       "      <td>Circle</td>\n",
       "      <td>Hamster</td>\n",
       "      <td>Finland</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Novice</td>\n",
       "      <td>Lava Hot</td>\n",
       "      <td>a</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Novice_Lava Hot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>Red</td>\n",
       "      <td>Triangle</td>\n",
       "      <td>Hamster</td>\n",
       "      <td>Costa Rica</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Grandmaster</td>\n",
       "      <td>Cold</td>\n",
       "      <td>h</td>\n",
       "      <td>C</td>\n",
       "      <td>OZ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Grandmaster_Cold</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  bin_0  bin_1  bin_2 bin_3 bin_4 nom_0      nom_1    nom_2       nom_3  \\\n",
       "0   0    0.0    0.0    0.0     F     N   Red  Trapezoid  Hamster      Russia   \n",
       "1   1    1.0    1.0    0.0     F     Y   Red       Star  Axolotl         NaN   \n",
       "2   2    0.0    1.0    0.0     F     N   Red        NaN  Hamster      Canada   \n",
       "3   3    NaN    0.0    0.0     F     N   Red     Circle  Hamster     Finland   \n",
       "4   4    0.0    NaN    0.0     T     N   Red   Triangle  Hamster  Costa Rica   \n",
       "\n",
       "   ... ord_0        ord_1     ord_2 ord_3 ord_4 ord_5  day month target  \\\n",
       "0  ...   3.0  Contributor       Hot     c     U    Pw  6.0   3.0      0   \n",
       "1  ...   3.0  Grandmaster      Warm     e     X    pE  7.0   7.0      0   \n",
       "2  ...   3.0          NaN  Freezing     n     P    eN  5.0   9.0      0   \n",
       "3  ...   1.0       Novice  Lava Hot     a     C   NaN  3.0   3.0      0   \n",
       "4  ...   3.0  Grandmaster      Cold     h     C    OZ  5.0  12.0      0   \n",
       "\n",
       "        new_feature  \n",
       "0   Contributor_Hot  \n",
       "1  Grandmaster_Warm  \n",
       "2      nan_Freezing  \n",
       "3   Novice_Lava Hot  \n",
       "4  Grandmaster_Cold  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                 Contributor_Hot_c\n",
       "1                Grandmaster_Warm_e\n",
       "2                    nan_Freezing_n\n",
       "3                 Novice_Lava Hot_a\n",
       "4                Grandmaster_Cold_h\n",
       "                    ...            \n",
       "599995            Novice_Freezing_a\n",
       "599996         Novice_Boiling Hot_n\n",
       "599997       Contributor_Freezing_n\n",
       "599998                Master_Warm_m\n",
       "599999    Contributor_Boiling Hot_b\n",
       "Name: new_feature, Length: 600000, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can also combine three or four or even more features\n",
    "df[\"new_feature\"] = (\n",
    "    df.ord_1.astype(str)\n",
    "    + \"_\"\n",
    "    + df.ord_2.astype(str)\n",
    "    + \"_\"\n",
    "    + df.ord_3.astype(str)\n",
    ")\n",
    "df.new_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling NaN values\n",
    "\n",
    "#### 1. Drop them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id            0\n",
       "bin_0     17894\n",
       "bin_1     18003\n",
       "bin_2     17930\n",
       "bin_3     18014\n",
       "bin_4     18047\n",
       "nom_0     18252\n",
       "nom_1     18156\n",
       "nom_2     18035\n",
       "nom_3     18121\n",
       "nom_4     18035\n",
       "nom_5     17778\n",
       "nom_6     18131\n",
       "nom_7     18003\n",
       "nom_8     17755\n",
       "nom_9     18073\n",
       "ord_0     18288\n",
       "ord_1     18041\n",
       "ord_2     18075\n",
       "ord_3     17916\n",
       "ord_4     17930\n",
       "ord_5     17713\n",
       "day       17952\n",
       "month     17988\n",
       "target        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv(\"cat_train.csv\")\n",
    "\n",
    "# View the number of NaN values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        0\n",
       "bin_0     0\n",
       "bin_1     0\n",
       "bin_2     0\n",
       "bin_3     0\n",
       "bin_4     0\n",
       "nom_0     0\n",
       "nom_1     0\n",
       "nom_2     0\n",
       "nom_3     0\n",
       "nom_4     0\n",
       "nom_5     0\n",
       "nom_6     0\n",
       "nom_7     0\n",
       "nom_8     0\n",
       "nom_9     0\n",
       "ord_0     0\n",
       "ord_1     0\n",
       "ord_2     0\n",
       "ord_3     0\n",
       "ord_4     0\n",
       "ord_5     0\n",
       "day       0\n",
       "month     0\n",
       "target    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop NaN values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# View number of NaN values\n",
    "df.isna().sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Treat NaN values as a completely new category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Freezing       142726\n",
       "Warm           124239\n",
       "Cold            97822\n",
       "Boiling Hot     84790\n",
       "Hot             67508\n",
       "Lava Hot        64840\n",
       "Name: ord_2, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv(\"cat_train.csv\")\n",
    "\n",
    "# View the value counts of ord_2 column\n",
    "df.ord_2.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Freezing       142726\n",
       "Warm           124239\n",
       "Cold            97822\n",
       "Boiling Hot     84790\n",
       "Hot             67508\n",
       "Lava Hot        64840\n",
       "NONE            18075\n",
       "Name: ord_2, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fill NaNs with None\n",
    "df.ord_2.fillna(\"NONE\").value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding the train and test data\n",
    "import pandas as pd \n",
    "from sklearn import preprocessing\n",
    "\n",
    "# read training data\n",
    "train = pd.read_csv(\"cat_train.csv\")\n",
    "\n",
    "# read test data\n",
    "test = pd.read_csv(\"cat_test.csv\")\n",
    "\n",
    "# create a fake target column for test data\n",
    "# since this column doesn't exist\n",
    "test.loc[:, \"target\"] = -1\n",
    "\n",
    "# concatenate both training and test data\n",
    "data = pd.concat([train, test]).reset_index(drop=True)\n",
    "\n",
    "# make a list of features we are interested in \n",
    "# id and target is something we should not encode\n",
    "features = [x for x in train.columns if x not in [\"id\",\"target\"]]\n",
    "\n",
    "# loop over the feature list\n",
    "for feat in features:\n",
    "    # create a new instance of LabelEncoder for each feature\n",
    "    lbl_enc = preprocessing.LabelEncoder()\n",
    "    \n",
    "    # note the trick here\n",
    "    # since its categorical data, we fillna with a string\n",
    "    # and we convert all the data to string type\n",
    "    # so, no matter its int or float, its converted to string\n",
    "    # int/float but categorical !!!\n",
    "    temp_col = data[feat].fillna(\"NONE\").astype(str).values\n",
    "    \n",
    "    # we can use fit_transform here as we do not have any extra test data that we need to transform on separately\n",
    "    data.loc[:, feat] = lbl_enc.fit_transform(temp_col)\n",
    "    \n",
    "# split the training and test data again\n",
    "train = data[data.target != -1].reset_index(drop=True)\n",
    "test = data[data.target == -1].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>bin_0</th>\n",
       "      <th>bin_1</th>\n",
       "      <th>bin_2</th>\n",
       "      <th>bin_3</th>\n",
       "      <th>bin_4</th>\n",
       "      <th>nom_0</th>\n",
       "      <th>nom_1</th>\n",
       "      <th>nom_2</th>\n",
       "      <th>nom_3</th>\n",
       "      <th>...</th>\n",
       "      <th>nom_9</th>\n",
       "      <th>ord_0</th>\n",
       "      <th>ord_1</th>\n",
       "      <th>ord_2</th>\n",
       "      <th>ord_3</th>\n",
       "      <th>ord_4</th>\n",
       "      <th>ord_5</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>528156</th>\n",
       "      <td>528156</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1708</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>159</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420837</th>\n",
       "      <td>420837</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1998</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>46</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113725</th>\n",
       "      <td>113725</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1002</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149574</th>\n",
       "      <td>149574</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>179</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63070</th>\n",
       "      <td>63070</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1216</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>23</td>\n",
       "      <td>135</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  bin_0  bin_1  bin_2  bin_3  bin_4  nom_0  nom_1  nom_2  nom_3  \\\n",
       "528156  528156      0      0      0      0      2      0      6      5      6   \n",
       "420837  420837      1      0      0      0      2      3      5      0      6   \n",
       "113725  113725      0      0      0      2      2      3      5      0      4   \n",
       "149574  149574      0      0      1      0      0      3      2      6      3   \n",
       "63070    63070      1      0      0      2      2      3      0      3      6   \n",
       "\n",
       "        ...  nom_9  ord_0  ord_1  ord_2  ord_3  ord_4  ord_5  day  month  \\\n",
       "528156  ...   1708      1      2      2      2     25    159    4      5   \n",
       "420837  ...   1998      3      5      2     11      2     46    7      3   \n",
       "113725  ...   1002      1      0      4      3     21     22    6      7   \n",
       "149574  ...    179      0      1      6      2     15     21    5      3   \n",
       "63070   ...   1216      0      1      2     14     23    135    6      5   \n",
       "\n",
       "        target  \n",
       "528156       0  \n",
       "420837       0  \n",
       "113725       0  \n",
       "149574       0  \n",
       "63070        0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View train data\n",
    "train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>bin_0</th>\n",
       "      <th>bin_1</th>\n",
       "      <th>bin_2</th>\n",
       "      <th>bin_3</th>\n",
       "      <th>bin_4</th>\n",
       "      <th>nom_0</th>\n",
       "      <th>nom_1</th>\n",
       "      <th>nom_2</th>\n",
       "      <th>nom_3</th>\n",
       "      <th>...</th>\n",
       "      <th>nom_9</th>\n",
       "      <th>ord_0</th>\n",
       "      <th>ord_1</th>\n",
       "      <th>ord_2</th>\n",
       "      <th>ord_3</th>\n",
       "      <th>ord_4</th>\n",
       "      <th>ord_5</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17163</th>\n",
       "      <td>617163</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>614</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>113</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276786</th>\n",
       "      <td>876786</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>705</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>182</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70615</th>\n",
       "      <td>670615</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>1965</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31183</th>\n",
       "      <td>631183</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1802</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>155</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68979</th>\n",
       "      <td>668979</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>1008</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>25</td>\n",
       "      <td>72</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  bin_0  bin_1  bin_2  bin_3  bin_4  nom_0  nom_1  nom_2  nom_3  \\\n",
       "17163   617163      0      1      1      0      2      2      5      0      6   \n",
       "276786  876786      0      0      1      2      0      0      0      2      2   \n",
       "70615   670615      0      0      0      0      2      1      2      3      6   \n",
       "31183   631183      0      0      0      2      0      0      2      3      2   \n",
       "68979   668979      0      0      0      2      2      3      0      3      3   \n",
       "\n",
       "        ...  nom_9  ord_0  ord_1  ord_2  ord_3  ord_4  ord_5  day  month  \\\n",
       "17163   ...    614      1      0      2      9     13    113    3      3   \n",
       "276786  ...    705      0      1      4     14     10    182    4     10   \n",
       "70615   ...   1965      2      5      2     13      4     76    0      0   \n",
       "31183   ...   1802      0      1      3      8      8    155    1      3   \n",
       "68979   ...   1008      1      1      6     13     25     72    6      5   \n",
       "\n",
       "        target  \n",
       "17163       -1  \n",
       "276786      -1  \n",
       "70615       -1  \n",
       "31183       -1  \n",
       "68979       -1  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View test data\n",
    "test.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N       39978\n",
       "P       37890\n",
       "Y       36657\n",
       "A       36633\n",
       "R       33045\n",
       "U       32897\n",
       "M       32504\n",
       "X       32347\n",
       "C       32112\n",
       "H       31189\n",
       "Q       30145\n",
       "T       29723\n",
       "O       25610\n",
       "B       25212\n",
       "E       21871\n",
       "K       21676\n",
       "I       19805\n",
       "NONE    17930\n",
       "D       17284\n",
       "F       16721\n",
       "W        8268\n",
       "Z        5790\n",
       "S        4595\n",
       "G        3404\n",
       "V        3107\n",
       "J        1950\n",
       "L        1657\n",
       "Name: ord_4, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View categories in ord_4 column\n",
    "df.ord_4.fillna(\"NONE\").value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We define a value `rare` if it is less than 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N       39978\n",
       "P       37890\n",
       "Y       36657\n",
       "A       36633\n",
       "R       33045\n",
       "U       32897\n",
       "M       32504\n",
       "X       32347\n",
       "C       32112\n",
       "H       31189\n",
       "Q       30145\n",
       "T       29723\n",
       "O       25610\n",
       "B       25212\n",
       "E       21871\n",
       "K       21676\n",
       "I       19805\n",
       "NONE    17930\n",
       "D       17284\n",
       "F       16721\n",
       "W        8268\n",
       "Z        5790\n",
       "S        4595\n",
       "RARE     3607\n",
       "G        3404\n",
       "V        3107\n",
       "Name: ord_4, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.ord_4 = df.ord_4.fillna(\"NONE\")\n",
    "\n",
    "df.loc[\n",
    "    df[\"ord_4\"].value_counts()[df[\"ord_4\"]].values < 2000, \"ord_4\"\n",
    "] = \"RARE\"\n",
    "\n",
    "df.ord_4.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `create_folds.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_folds.py\n",
    "import pandas as pd \n",
    "from sklearn import model_selection\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # read training data\n",
    "    df = pd.read_csv(\"cat_train.csv\")\n",
    "    \n",
    "    # we create a new column called kfold and fill it with -1\n",
    "    df[\"kfold\"] = -1 \n",
    "    \n",
    "    # the next step is to randomize the rows of the data\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # fetch labels\n",
    "    y = df.target.values\n",
    "    \n",
    "    # initiate the kfold class from model_selection module\n",
    "    kf = model_selection.StratifiedKFold(n_splits=5)\n",
    "    \n",
    "    # fill the new kfold column\n",
    "    for f,(t_, v_) in enumerate(kf.split(X=df, y=y)):\n",
    "        df.loc[v_, 'kfold'] = f\n",
    "        \n",
    "    # save the new csv with kfold column\n",
    "    df.to_csv(\"cat_train_folds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>bin_0</th>\n",
       "      <th>bin_1</th>\n",
       "      <th>bin_2</th>\n",
       "      <th>bin_3</th>\n",
       "      <th>bin_4</th>\n",
       "      <th>nom_0</th>\n",
       "      <th>nom_1</th>\n",
       "      <th>nom_2</th>\n",
       "      <th>nom_3</th>\n",
       "      <th>...</th>\n",
       "      <th>ord_0</th>\n",
       "      <th>ord_1</th>\n",
       "      <th>ord_2</th>\n",
       "      <th>ord_3</th>\n",
       "      <th>ord_4</th>\n",
       "      <th>ord_5</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>target</th>\n",
       "      <th>kfold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>397030</th>\n",
       "      <td>377488</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Circle</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Costa Rica</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Expert</td>\n",
       "      <td>Freezing</td>\n",
       "      <td>i</td>\n",
       "      <td>F</td>\n",
       "      <td>DI</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441118</th>\n",
       "      <td>64968</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>F</td>\n",
       "      <td>Y</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Polygon</td>\n",
       "      <td>Snake</td>\n",
       "      <td>India</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Novice</td>\n",
       "      <td>Cold</td>\n",
       "      <td>c</td>\n",
       "      <td>G</td>\n",
       "      <td>qN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483088</th>\n",
       "      <td>217612</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Square</td>\n",
       "      <td>Hamster</td>\n",
       "      <td>Finland</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hot</td>\n",
       "      <td>o</td>\n",
       "      <td>K</td>\n",
       "      <td>Hk</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224420</th>\n",
       "      <td>351617</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Triangle</td>\n",
       "      <td>Axolotl</td>\n",
       "      <td>India</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Contributor</td>\n",
       "      <td>Freezing</td>\n",
       "      <td>h</td>\n",
       "      <td>C</td>\n",
       "      <td>Ty</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35462</th>\n",
       "      <td>563802</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>T</td>\n",
       "      <td>N</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Triangle</td>\n",
       "      <td>Dog</td>\n",
       "      <td>Finland</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cold</td>\n",
       "      <td>m</td>\n",
       "      <td>P</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  bin_0  bin_1  bin_2 bin_3 bin_4 nom_0     nom_1    nom_2  \\\n",
       "397030  377488    NaN    1.0    1.0     F     N  Blue    Circle      Dog   \n",
       "441118   64968    0.0    0.0    0.0     F     Y  Blue   Polygon    Snake   \n",
       "483088  217612    0.0    0.0    0.0     T     N  Blue    Square  Hamster   \n",
       "224420  351617    0.0    1.0    1.0     T     N  Blue  Triangle  Axolotl   \n",
       "35462   563802    0.0    1.0    0.0     T     N  Blue  Triangle      Dog   \n",
       "\n",
       "             nom_3  ... ord_0        ord_1     ord_2 ord_3 ord_4 ord_5  day  \\\n",
       "397030  Costa Rica  ...   2.0       Expert  Freezing     i     F    DI  1.0   \n",
       "441118       India  ...   1.0       Novice      Cold     c     G    qN  1.0   \n",
       "483088     Finland  ...   2.0          NaN       Hot     o     K    Hk  5.0   \n",
       "224420       India  ...   1.0  Contributor  Freezing     h     C    Ty  5.0   \n",
       "35462      Finland  ...   3.0          NaN      Cold     m     P   NaN  6.0   \n",
       "\n",
       "       month target kfold  \n",
       "397030   3.0      0     3  \n",
       "441118  12.0      0     3  \n",
       "483088   5.0      0     4  \n",
       "224420   6.0      0     1  \n",
       "35462    3.0      0     0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    120000\n",
       "3    120000\n",
       "2    120000\n",
       "1    120000\n",
       "0    120000\n",
       "Name: kfold, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check our new folds csv to see the number of samples per fold\n",
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv(\"cat_train_folds.csv\")\n",
    "\n",
    "df.kfold.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    97536\n",
       "1    22464\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check target distribution per fold\n",
    "df[df.kfold==0].target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    97536\n",
       "1    22464\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.kfold==1].target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    97535\n",
       "1    22465\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.kfold==2].target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    97535\n",
       "1    22465\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.kfold==3].target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    97535\n",
       "1    22465\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.kfold==4].target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Simple Model*: One Hot Encoding + Logistic Regression\n",
    "\n",
    "### `ohe_logres.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7876915259114206\n"
     ]
    }
   ],
   "source": [
    "# ohe_logres.py\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "\n",
    "def run(fold):\n",
    "    # load the full training data with folds\n",
    "    df = pd.read_csv(\"cat_train_folds.csv\")\n",
    "    \n",
    "    # all columns are features except id, target and kfold columns\n",
    "    features = [x for x in df.columns if x not in (\"id\", \"target\", \"kfold\")]\n",
    "    \n",
    "    # fill all NaN values with NONE\n",
    "    # note that all columns are converted to \"strings\"\n",
    "    # it doesn't matter because all are categories\n",
    "    for col in features:\n",
    "        df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
    "        \n",
    "    # get training data using folds\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    \n",
    "    # get validation data using folds\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "    \n",
    "    # initialize OneHotEncoder from scikit-learn\n",
    "    ohe = preprocessing.OneHotEncoder()\n",
    "    \n",
    "    # fit ohe on training + validation features\n",
    "    full_data = pd.concat(\n",
    "        [df_train[features], df_valid[features]], axis=0\n",
    "    )\n",
    "    ohe.fit(full_data[features])\n",
    "    \n",
    "    # transform training data\n",
    "    x_train = ohe.transform(df_train[features])\n",
    "    \n",
    "    # transform validation data\n",
    "    x_valid = ohe.transform(df_valid[features])\n",
    "    \n",
    "    # initialize Logistic Regression model\n",
    "    model = linear_model.LogisticRegression()\n",
    "    \n",
    "    # fit model on training data (ohe)\n",
    "    model.fit(x_train, df_train.target.values)\n",
    "    \n",
    "    # prediction on validation data\n",
    "    # we need probability values as we are calculating AUC\n",
    "    # we will use the probability of 1s\n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1]\n",
    "    \n",
    "    # get roc_auc score\n",
    "    auc = metrics.roc_auc_score(df_valid.target.values, valid_preds)\n",
    "    \n",
    "    # print auc\n",
    "    print(auc)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # run function for fold = 0\n",
    "    # we can just replace this number and run this for any fold\n",
    "    run(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold = 0, AUC = 0.7876915259114206\n",
      "Fold = 1, AUC = 0.7854353428375456\n",
      "Fold = 2, AUC = 0.7871695062046414\n",
      "Fold = 3, AUC = 0.7849085782477077\n",
      "Fold = 4, AUC = 0.7856094501096818\n"
     ]
    }
   ],
   "source": [
    "# Change the code to run it for all folds\n",
    "# ohe_logres.py\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "\n",
    "def run(fold):\n",
    "    # load the full training data with folds\n",
    "    df = pd.read_csv(\"cat_train_folds.csv\")\n",
    "    \n",
    "    # all columns are features except id, target and kfold columns\n",
    "    features = [x for x in df.columns if x not in (\"id\", \"target\", \"kfold\")]\n",
    "    \n",
    "    # fill all NaN values with NONE\n",
    "    # note that all columns are converted to \"strings\"\n",
    "    # it doesn't matter because all are categories\n",
    "    for col in features:\n",
    "        df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
    "        \n",
    "    # get training data using folds\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    \n",
    "    # get validation data using folds\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "    \n",
    "    # initialize OneHotEncoder from scikit-learn\n",
    "    ohe = preprocessing.OneHotEncoder()\n",
    "    \n",
    "    # fit ohe on training + validation features\n",
    "    full_data = pd.concat(\n",
    "        [df_train[features], df_valid[features]], axis=0\n",
    "    )\n",
    "    ohe.fit(full_data[features])\n",
    "    \n",
    "    # transform training data\n",
    "    x_train = ohe.transform(df_train[features])\n",
    "    \n",
    "    # transform validation data\n",
    "    x_valid = ohe.transform(df_valid[features])\n",
    "    \n",
    "    # initialize Logistic Regression model\n",
    "    model = linear_model.LogisticRegression()\n",
    "    \n",
    "    # fit model on training data (ohe)\n",
    "    model.fit(x_train, df_train.target.values)\n",
    "    \n",
    "    # prediction on validation data\n",
    "    # we need probability values as we are calculating AUC\n",
    "    # we will use the probability of 1s\n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1]\n",
    "    \n",
    "    # get roc_auc score\n",
    "    auc = metrics.roc_auc_score(df_valid.target.values, valid_preds)\n",
    "    \n",
    "    # print auc\n",
    "    print(f\"Fold = {fold}, AUC = {auc}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for fold_ in range(5):\n",
    "        run(fold_)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Another Model*: Label Encoding + Random Forest\n",
    "\n",
    "### `lbl_rf.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold = 0, AUC = 0.7165689631333727\n",
      "Fold = 1, AUC = 0.7142999617684446\n",
      "Fold = 2, AUC = 0.7180122804792258\n",
      "Fold = 3, AUC = 0.7164325180579998\n",
      "Fold = 4, AUC = 0.7161191295548788\n"
     ]
    }
   ],
   "source": [
    "# lbl_rf.py\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn import ensemble\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def run(fold):\n",
    "    # load the full training data with folds\n",
    "    df = pd.read_csv(\"cat_train_folds.csv\")\n",
    "    \n",
    "    # all columns are features except id, target and kfold columns\n",
    "    features = [x for x in df.columns if x not in (\"id\", \"target\", \"kfold\")]\n",
    "    \n",
    "    # fill all NaN values with NONE\n",
    "    # note that all columns are converted to \"strings\"\n",
    "    # it doesn't matter because all are categories\n",
    "    for col in features:\n",
    "        df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
    "        \n",
    "    # now its time to label encode the features\n",
    "    for col in features:\n",
    "        # initialize LabelEncoder for each column\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        \n",
    "        # fit label encoder on all data\n",
    "        lbl.fit(df[col])\n",
    "        \n",
    "        # transform all the data\n",
    "        df.loc[:, col] = lbl.transform(df[col])\n",
    "        \n",
    "    # get training data using folds\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    \n",
    "    # get validation data using folds\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "      \n",
    "    # get training data\n",
    "    x_train = df_train[features].values\n",
    "    \n",
    "    # get validation data\n",
    "    x_valid = df_valid[features].values\n",
    "    \n",
    "    # initialize Random Forest model\n",
    "    model = ensemble.RandomForestClassifier(n_jobs=-1)\n",
    "    \n",
    "    # fit model on training data\n",
    "    model.fit(x_train, df_train.target.values)\n",
    "    \n",
    "    # prediction on validation data\n",
    "    # we need probability values as we are calculating AUC\n",
    "    # we will use the probability of 1s\n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1]\n",
    "    \n",
    "    # get roc_auc score\n",
    "    auc = metrics.roc_auc_score(df_valid.target.values, valid_preds)\n",
    "    \n",
    "    # print auc\n",
    "    print(f\"Fold = {fold}, AUC = {auc}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for fold_ in range(5):\n",
    "        run(fold_)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Another model*: One Hot Encoding + fitting TruncatedSVD on sparse matrix with training + validation data\n",
    "\n",
    "### `ohe_svd_rf.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold = 0, AUC = 0.7086326418328672\n",
      "Fold = 1, AUC = 0.7052239907671172\n",
      "Fold = 2, AUC = 0.7081827246386388\n",
      "Fold = 3, AUC = 0.7067629228294052\n",
      "Fold = 4, AUC = 0.7095767554254209\n"
     ]
    }
   ],
   "source": [
    "# ohe_svd_rf.py\n",
    "\n",
    "# lbl_rf.py\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "from scipy import sparse\n",
    "from sklearn import decomposition\n",
    "from sklearn import ensemble\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def run(fold):\n",
    "    # load the full training data with folds\n",
    "    df = pd.read_csv(\"cat_train_folds.csv\")\n",
    "    \n",
    "    # all columns are features except id, target and kfold columns\n",
    "    features = [x for x in df.columns if x not in (\"id\", \"target\", \"kfold\")]\n",
    "    \n",
    "    # fill all NaN values with NONE\n",
    "    # note that all columns are converted to \"strings\"\n",
    "    # it doesn't matter because all are categories\n",
    "    for col in features:\n",
    "        df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
    "        \n",
    "    # get training data using folds\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    \n",
    "    # get validation data using folds\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "    # initialize OneHotEncoder from scikit-learn\n",
    "    ohe = preprocessing.OneHotEncoder()\n",
    "\n",
    "    # fit ohe on training + validation features\n",
    "    full_data = pd.concat(\n",
    "        [df_train[features], df_valid[features]], axis=0\n",
    "    )\n",
    "    ohe.fit(full_data[features])\n",
    "\n",
    "    # transform training data\n",
    "    x_train = ohe.transform(df_train[features])\n",
    "    \n",
    "    # transform validation data\n",
    "    x_valid = ohe.transform(df_valid[features])\n",
    "    \n",
    "    # initialize Truncated SVD\n",
    "    # we are reducing the data to 120 components\n",
    "    svd = decomposition.TruncatedSVD(n_components=120)\n",
    "    \n",
    "    # fit SVD on full sparse training data\n",
    "    full_sparse = sparse.vstack((x_train, x_valid))\n",
    "    svd.fit(full_sparse)\n",
    "\n",
    "    # transform sparse training data\n",
    "    x_train = svd.transform(x_train)\n",
    "    \n",
    "    # transform sparse validation data\n",
    "    x_valid = svd.transform(x_valid)\n",
    "    \n",
    "    # initialize Random Forest model\n",
    "    model = ensemble.RandomForestClassifier(n_jobs=-1)\n",
    "    \n",
    "    # fit model on training data (ohe)\n",
    "    model.fit(x_train, df_train.target.values)\n",
    "    \n",
    "    # prediction on validation data\n",
    "    # we need probability values as we are calculating AUC\n",
    "    # we will use the probability of 1s\n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1]\n",
    "    \n",
    "    # get roc_auc score\n",
    "    auc = metrics.roc_auc_score(df_valid.target.values, valid_preds)\n",
    "    \n",
    "    # print auc\n",
    "    print(f\"Fold = {fold}, AUC = {auc}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for fold_ in range(5):\n",
    "        run(fold_)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Another Model*: Label Encoding + XGBoost\n",
    "\n",
    "### `lbl_xgb.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:02:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fold = 0, AUC = 0.7623592364471694\n",
      "[20:05:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fold = 1, AUC = 0.7614065212034647\n",
      "[20:07:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fold = 2, AUC = 0.7627550141479342\n",
      "[20:10:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fold = 3, AUC = 0.760773772581606\n",
      "[20:13:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fold = 4, AUC = 0.7634291002113744\n"
     ]
    }
   ],
   "source": [
    "# lbl_rf.py\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def run(fold):\n",
    "    # load the full training data with folds\n",
    "    df = pd.read_csv(\"cat_train_folds.csv\")\n",
    "    \n",
    "    # all columns are features except id, target and kfold columns\n",
    "    features = [x for x in df.columns if x not in (\"id\", \"target\", \"kfold\")]\n",
    "    \n",
    "    # fill all NaN values with NONE\n",
    "    # note that all columns are converted to \"strings\"\n",
    "    # it doesn't matter because all are categories\n",
    "    for col in features:\n",
    "        df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
    "        \n",
    "    # now its time to label encode the features\n",
    "    for col in features:\n",
    "        # initialize LabelEncoder for each column\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        \n",
    "        # fit label encoder on all data\n",
    "        lbl.fit(df[col])\n",
    "        \n",
    "        # transform all the data\n",
    "        df.loc[:, col] = lbl.transform(df[col])\n",
    "        \n",
    "    # get training data using folds\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    \n",
    "    # get validation data using folds\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "      \n",
    "    # get training data\n",
    "    x_train = df_train[features].values\n",
    "    \n",
    "    # get validation data\n",
    "    x_valid = df_valid[features].values\n",
    "    \n",
    "    # initialize XGBoost model\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_jobs=-1,\n",
    "        max_depth=7,\n",
    "        n_estimators=200\n",
    "    )\n",
    "    \n",
    "    # fit model on training data \n",
    "    model.fit(x_train, df_train.target.values)\n",
    "    \n",
    "    # prediction on validation data\n",
    "    # we need probability values as we are calculating AUC\n",
    "    # we will use the probability of 1s\n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1]\n",
    "    \n",
    "    # get roc_auc score\n",
    "    auc = metrics.roc_auc_score(df_valid.target.values, valid_preds)\n",
    "    \n",
    "    # print auc\n",
    "    print(f\"Fold = {fold}, AUC = {auc}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for fold_ in range(5):\n",
    "        run(fold_)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, change the dataset and use US Adult Census Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<=50K    24720\n",
       ">50K      7841\n",
       "Name: income, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv(\"adult.csv\")\n",
    "\n",
    "df.income.value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<=50K    0.75919\n",
       ">50K     0.24081\n",
       "Name: income, dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.income.value_counts(normalize=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- About 24% of the total number of samples have income greater than 50K USD.\n",
    "- So, we will use AUC as our metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education.num</th>\n",
       "      <th>marital.status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital.gain</th>\n",
       "      <th>capital.loss</th>\n",
       "      <th>hours.per.week</th>\n",
       "      <th>native.country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90</td>\n",
       "      <td>?</td>\n",
       "      <td>77053</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>?</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>Private</td>\n",
       "      <td>132870</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>18</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66</td>\n",
       "      <td>?</td>\n",
       "      <td>186061</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>?</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>4356</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>Private</td>\n",
       "      <td>140359</td>\n",
       "      <td>7th-8th</td>\n",
       "      <td>4</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>Private</td>\n",
       "      <td>264663</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>3900</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age workclass  fnlwgt     education  education.num marital.status  \\\n",
       "0   90         ?   77053       HS-grad              9        Widowed   \n",
       "1   82   Private  132870       HS-grad              9        Widowed   \n",
       "2   66         ?  186061  Some-college             10        Widowed   \n",
       "3   54   Private  140359       7th-8th              4       Divorced   \n",
       "4   41   Private  264663  Some-college             10      Separated   \n",
       "\n",
       "          occupation   relationship   race     sex  capital.gain  \\\n",
       "0                  ?  Not-in-family  White  Female             0   \n",
       "1    Exec-managerial  Not-in-family  White  Female             0   \n",
       "2                  ?      Unmarried  Black  Female             0   \n",
       "3  Machine-op-inspct      Unmarried  White  Female             0   \n",
       "4     Prof-specialty      Own-child  White  Female             0   \n",
       "\n",
       "   capital.loss  hours.per.week native.country income  \n",
       "0          4356              40  United-States  <=50K  \n",
       "1          4356              18  United-States  <=50K  \n",
       "2          4356              40  United-States  <=50K  \n",
       "3          3900              40  United-States  <=50K  \n",
       "4          3900              40  United-States  <=50K  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create Folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `create_folds.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_folds.py\n",
    "import pandas as pd \n",
    "from sklearn import model_selection\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # read training data\n",
    "    df = pd.read_csv(\"adult.csv\")\n",
    "    \n",
    "    # we create a new column called kfold and fill it with -1\n",
    "    df[\"kfold\"] = -1 \n",
    "    \n",
    "    # the next step is to randomize the rows of the data\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # fetch labels\n",
    "    y = df.income.values\n",
    "    \n",
    "    # initiate the kfold class from model_selection module\n",
    "    kf = model_selection.StratifiedKFold(n_splits=5)\n",
    "    \n",
    "    # fill the new kfold column\n",
    "    for f,(t_, v_) in enumerate(kf.split(X=df, y=y)):\n",
    "        df.loc[v_, 'kfold'] = f\n",
    "        \n",
    "    # save the new csv with kfold column\n",
    "    df.to_csv(\"adult_folds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education.num</th>\n",
       "      <th>marital.status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital.gain</th>\n",
       "      <th>capital.loss</th>\n",
       "      <th>hours.per.week</th>\n",
       "      <th>native.country</th>\n",
       "      <th>income</th>\n",
       "      <th>kfold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>Private</td>\n",
       "      <td>237943</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>99999</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>Federal-gov</td>\n",
       "      <td>199925</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>Private</td>\n",
       "      <td>175465</td>\n",
       "      <td>10th</td>\n",
       "      <td>6</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>Private</td>\n",
       "      <td>118712</td>\n",
       "      <td>Assoc-voc</td>\n",
       "      <td>11</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Craft-repair</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>1504</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35</td>\n",
       "      <td>Private</td>\n",
       "      <td>215323</td>\n",
       "      <td>Assoc-voc</td>\n",
       "      <td>11</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age    workclass  fnlwgt  education  education.num      marital.status  \\\n",
       "0   39      Private  237943  Bachelors             13  Married-civ-spouse   \n",
       "1   46  Federal-gov  199925  Bachelors             13            Divorced   \n",
       "2   17      Private  175465       10th              6       Never-married   \n",
       "3   21      Private  118712  Assoc-voc             11       Never-married   \n",
       "4   35      Private  215323  Assoc-voc             11            Divorced   \n",
       "\n",
       "        occupation   relationship   race     sex  capital.gain  capital.loss  \\\n",
       "0  Exec-managerial        Husband  White    Male         99999             0   \n",
       "1   Prof-specialty  Not-in-family  White    Male             0             0   \n",
       "2    Other-service      Own-child  White  Female             0             0   \n",
       "3     Craft-repair      Own-child  White    Male             0          1504   \n",
       "4    Other-service      Unmarried  White  Female             0             0   \n",
       "\n",
       "   hours.per.week native.country income  kfold  \n",
       "0              70  United-States   >50K      0  \n",
       "1              40  United-States  <=50K      0  \n",
       "2              14  United-States  <=50K      0  \n",
       "3              40  United-States  <=50K      0  \n",
       "4              35  United-States  <=50K      0  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Simple Model*: One Hot Encoding + Logistic Regression\n",
    "\n",
    "### `ohe_logres.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold = 0, AUC = 0.9281433250622395\n",
      "Fold = 1, AUC = 0.9274763189559805\n",
      "Fold = 2, AUC = 0.9257037235403871\n",
      "Fold = 3, AUC = 0.9215594763390793\n",
      "Fold = 4, AUC = 0.9199870823632852\n"
     ]
    }
   ],
   "source": [
    "# ohe_logres.py\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "\n",
    "def run(fold):\n",
    "    # load the full training data with folds\n",
    "    df = pd.read_csv(\"adult_folds.csv\")\n",
    "    \n",
    "    # all columns are features except id, target and kfold columns\n",
    "    features = [x for x in df.columns if x not in (\"id\", \"income\", \"kfold\")]\n",
    "    \n",
    "    # fill all NaN values with NONE\n",
    "    # note that all columns are converted to \"strings\"\n",
    "    # it doesn't matter because all are categories\n",
    "    for col in features:\n",
    "        df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
    "        \n",
    "    # get training data using folds\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    \n",
    "    # get validation data using folds\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "    \n",
    "    # initialize OneHotEncoder from scikit-learn\n",
    "    ohe = preprocessing.OneHotEncoder()\n",
    "    \n",
    "    # fit ohe on training + validation features\n",
    "    full_data = pd.concat(\n",
    "        [df_train[features], df_valid[features]], axis=0\n",
    "    )\n",
    "    ohe.fit(full_data[features])\n",
    "    \n",
    "    # transform training data\n",
    "    x_train = ohe.transform(df_train[features])\n",
    "    \n",
    "    # transform validation data\n",
    "    x_valid = ohe.transform(df_valid[features])\n",
    "    \n",
    "    # initialize Logistic Regression model\n",
    "    model = linear_model.LogisticRegression()\n",
    "    \n",
    "    # fit model on training data (ohe)\n",
    "    model.fit(x_train, df_train.income.values)\n",
    "    \n",
    "    # prediction on validation data\n",
    "    # we need probability values as we are calculating AUC\n",
    "    # we will use the probability of 1s\n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1]\n",
    "    \n",
    "    # get roc_auc score\n",
    "    auc = metrics.roc_auc_score(df_valid.income.values, valid_preds)\n",
    "    \n",
    "    # print auc\n",
    "    print(f\"Fold = {fold}, AUC = {auc}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for fold_ in range(5):\n",
    "        run(fold_)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Another Model*: Label Encoding + XGBoost\n",
    "\n",
    "### `lbl_xgb.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:53:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fold = 0, AUC = 0.8823870046883284\n",
      "[22:53:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fold = 1, AUC = 0.8735180965590119\n",
      "[22:53:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fold = 2, AUC = 0.8769075894921075\n",
      "[22:53:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fold = 3, AUC = 0.8682344296942077\n",
      "[22:53:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fold = 4, AUC = 0.8705387456863813\n"
     ]
    }
   ],
   "source": [
    "# lbl_xgb.py\n",
    "\n",
    "import pandas as pd \n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def run(fold):\n",
    "    # load the full training data with folds\n",
    "    df = pd.read_csv(\"adult_folds.csv\")\n",
    "    \n",
    "    # list of numerical columns\n",
    "    num_cols = [\n",
    "        \"fnlwgt\",\n",
    "        \"age\",\n",
    "        \"capital.gain\",\n",
    "        \"capital.loss\",\n",
    "        \"hours.per.week\"\n",
    "    ]\n",
    "    \n",
    "    # drop numerical columns\n",
    "    df = df.drop(num_cols, axis=1)\n",
    "    \n",
    "    # map targets to 0s and 1s\n",
    "    target_mapping = {\n",
    "        \"<=50K\": 0,\n",
    "        \">50K\": 1\n",
    "    }\n",
    "    df.loc[:, \"income\"] = df.income.map(target_mapping)\n",
    "    \n",
    "    # all columns except kfold & income columns\n",
    "    features = [f for f in df.columns if f not in (\"kfold\", \"income\")]\n",
    "    \n",
    "    # fill all NaN values with NONE\n",
    "    # note that all columns are converted to \"strings\"\n",
    "    # it doesn't matter because all are categories\n",
    "    for col in features:\n",
    "        df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
    "        \n",
    "    # now its time to label encode the features\n",
    "    for col in features:\n",
    "        # initialize LabelEncoder for each column\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        \n",
    "        # fit label encoder on all data\n",
    "        lbl.fit(df[col])\n",
    "        \n",
    "        # transform all the data\n",
    "        df.loc[:, col] = lbl.transform(df[col])\n",
    "        \n",
    "    # get training data using folds\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    \n",
    "    # get validation data using folds\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "      \n",
    "    # get training data\n",
    "    x_train = df_train[features].values\n",
    "    \n",
    "    # get validation data\n",
    "    x_valid = df_valid[features].values\n",
    "    \n",
    "    # initialize XGBoost model\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # fit model on training data \n",
    "    model.fit(x_train, df_train.income.values)\n",
    "    \n",
    "    # prediction on validation data\n",
    "    # we need probability values as we are calculating AUC\n",
    "    # we will use the probability of 1s\n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1]\n",
    "    \n",
    "    # get roc_auc score\n",
    "    auc = metrics.roc_auc_score(df_valid.income.values, valid_preds)\n",
    "        \n",
    "    # print auc\n",
    "    print(f\"Fold = {fold}, AUC = {auc}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for fold_ in range(5):\n",
    "        run(fold_)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:55:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fold = 0, AUC = 0.8770177550064868\n",
      "[22:55:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fold = 1, AUC = 0.8658947043623276\n",
      "[22:55:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fold = 2, AUC = 0.8690370413942277\n",
      "[22:55:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fold = 3, AUC = 0.8602539771976753\n",
      "[22:55:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fold = 4, AUC = 0.8621421528259362\n"
     ]
    }
   ],
   "source": [
    "# Increase max_depth to 7 and n_estimators to 200\n",
    "# lbl_xgb.py\n",
    "\n",
    "import pandas as pd \n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def run(fold):\n",
    "    # load the full training data with folds\n",
    "    df = pd.read_csv(\"adult_folds.csv\")\n",
    "    \n",
    "    # list of numerical columns\n",
    "    num_cols = [\n",
    "        \"fnlwgt\",\n",
    "        \"age\",\n",
    "        \"capital.gain\",\n",
    "        \"capital.loss\",\n",
    "        \"hours.per.week\"\n",
    "    ]\n",
    "    \n",
    "    # drop numerical columns\n",
    "    df = df.drop(num_cols, axis=1)\n",
    "    \n",
    "    # map targets to 0s and 1s\n",
    "    target_mapping = {\n",
    "        \"<=50K\": 0,\n",
    "        \">50K\": 1\n",
    "    }\n",
    "    df.loc[:, \"income\"] = df.income.map(target_mapping)\n",
    "    \n",
    "    # all columns except kfold & income columns\n",
    "    features = [f for f in df.columns if f not in (\"kfold\", \"income\")]\n",
    "    \n",
    "    # fill all NaN values with NONE\n",
    "    # note that all columns are converted to \"strings\"\n",
    "    # it doesn't matter because all are categories\n",
    "    for col in features:\n",
    "        df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
    "        \n",
    "    # now its time to label encode the features\n",
    "    for col in features:\n",
    "        # initialize LabelEncoder for each column\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        \n",
    "        # fit label encoder on all data\n",
    "        lbl.fit(df[col])\n",
    "        \n",
    "        # transform all the data\n",
    "        df.loc[:, col] = lbl.transform(df[col])\n",
    "        \n",
    "    # get training data using folds\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    \n",
    "    # get validation data using folds\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "      \n",
    "    # get training data\n",
    "    x_train = df_train[features].values\n",
    "    \n",
    "    # get validation data\n",
    "    x_valid = df_valid[features].values\n",
    "    \n",
    "    # initialize XGBoost model\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_jobs=-1,\n",
    "        max_depth=7,\n",
    "        n_estimators=200\n",
    "    )\n",
    "    \n",
    "    # fit model on training data \n",
    "    model.fit(x_train, df_train.income.values)\n",
    "    \n",
    "    # prediction on validation data\n",
    "    # we need probability values as we are calculating AUC\n",
    "    # we will use the probability of 1s\n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1]\n",
    "    \n",
    "    # get roc_auc score\n",
    "    auc = metrics.roc_auc_score(df_valid.income.values, valid_preds)\n",
    "        \n",
    "    # print auc\n",
    "    print(f\"Fold = {fold}, AUC = {auc}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for fold_ in range(5):\n",
    "        run(fold_)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:00:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fold = 0, AUC = 0.9280679750877128\n",
      "[23:00:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fold = 1, AUC = 0.9262547289850407\n",
      "[23:01:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fold = 2, AUC = 0.926675319703124\n",
      "[23:01:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fold = 3, AUC = 0.9236557866471831\n",
      "[23:01:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fold = 4, AUC = 0.9222465723243182\n"
     ]
    }
   ],
   "source": [
    "# Now, we include the numerical columns in the XGBoost model\n",
    "# lbl_xgb_num.py\n",
    "\n",
    "import pandas as pd \n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def run(fold):\n",
    "    # load the full training data with folds\n",
    "    df = pd.read_csv(\"adult_folds.csv\")\n",
    "    \n",
    "    # list of numerical columns\n",
    "    num_cols = [\n",
    "        \"fnlwgt\",\n",
    "        \"age\",\n",
    "        \"capital.gain\",\n",
    "        \"capital.loss\",\n",
    "        \"hours.per.week\"\n",
    "    ]\n",
    "    \n",
    "    # map targets to 0s and 1s\n",
    "    target_mapping = {\n",
    "        \"<=50K\": 0,\n",
    "        \">50K\": 1\n",
    "    }\n",
    "    df.loc[:, \"income\"] = df.income.map(target_mapping)\n",
    "    \n",
    "    # all columns except kfold & income columns\n",
    "    features = [f for f in df.columns if f not in (\"kfold\", \"income\")]\n",
    "    \n",
    "    # fill all NaN values with NONE\n",
    "    # note that all columns are converted to \"strings\"\n",
    "    # it doesn't matter because all are categories\n",
    "    for col in features:\n",
    "        # do not encode the numerical columns\n",
    "        if col not in num_cols:\n",
    "            df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
    "        \n",
    "    # now its time to label encode the features\n",
    "    for col in features:\n",
    "        if col not in num_cols:\n",
    "\n",
    "            # initialize LabelEncoder for each column\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "\n",
    "            # fit label encoder on all data\n",
    "            lbl.fit(df[col])\n",
    "\n",
    "            # transform all the data\n",
    "            df.loc[:, col] = lbl.transform(df[col])\n",
    "        \n",
    "    # get training data using folds\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    \n",
    "    # get validation data using folds\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "      \n",
    "    # get training data\n",
    "    x_train = df_train[features].values\n",
    "    \n",
    "    # get validation data\n",
    "    x_valid = df_valid[features].values\n",
    "    \n",
    "    # initialize XGBoost model\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # fit model on training data \n",
    "    model.fit(x_train, df_train.income.values)\n",
    "    \n",
    "    # prediction on validation data\n",
    "    # we need probability values as we are calculating AUC\n",
    "    # we will use the probability of 1s\n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1]\n",
    "    \n",
    "    # get roc_auc score\n",
    "    auc = metrics.roc_auc_score(df_valid.income.values, valid_preds)\n",
    "        \n",
    "    # print auc\n",
    "    print(f\"Fold = {fold}, AUC = {auc}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for fold_ in range(5):\n",
    "        run(fold_)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we take all the categorical columns and create all combinations of degree two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:15:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fold = 0, AUC = 0.9280909861577779\n",
      "[23:15:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fold = 1, AUC = 0.9258767068720691\n",
      "[23:15:23] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fold = 2, AUC = 0.9256503192903375\n",
      "[23:15:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fold = 3, AUC = 0.923145479368932\n",
      "[23:15:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fold = 4, AUC = 0.9215265179190607\n"
     ]
    }
   ],
   "source": [
    "# lbl_xgb_num_feat.py\n",
    "import itertools\n",
    "import pandas as pd \n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def feature_engineering(df, cat_cols):\n",
    "    \"\"\"\n",
    "    This function is used for feature engineering\n",
    "    :param df: the pandas dataframe with train/test data\n",
    "    :param cat-cols: list if categorical columns\n",
    "    :return: dataframe with new features\n",
    "    \"\"\"\n",
    "    # this will create all 2-combinations of values in this list\n",
    "    # for example:\n",
    "    # list(itertools.combinations([1,2,3], 2)) will return\n",
    "    # [(1,2), (1,3), (2,3)]\n",
    "    combi = list(itertools.combinations(cat_cols, 2))\n",
    "    for c1, c2 in combi:\n",
    "        df.loc[:, c1 + \"_\" + c2] = df[c1].astype(str) + \"_\" + df[c2].astype(str)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def run(fold):\n",
    "    # load the full training data with folds\n",
    "    df = pd.read_csv(\"adult_folds.csv\")\n",
    "    \n",
    "    # list of numerical columns\n",
    "    num_cols = [\n",
    "        \"fnlwgt\",\n",
    "        \"age\",\n",
    "        \"capital.gain\",\n",
    "        \"capital.loss\",\n",
    "        \"hours.per.week\"\n",
    "    ]\n",
    "    \n",
    "    # map targets to 0s and 1s\n",
    "    target_mapping = {\n",
    "        \"<=50K\": 0,\n",
    "        \">50K\": 1\n",
    "    }\n",
    "    df.loc[:, \"income\"] = df.income.map(target_mapping)\n",
    "    \n",
    "    # list of categorical columns for feature engineering \n",
    "    cat_cols = [c for c in df.columns if c not in num_cols and c not in (\"kfold\", \"income\")]\n",
    "    \n",
    "    # add new features\n",
    "    df = feature_engineering(df, cat_cols)\n",
    "    \n",
    "    # all columns are features except kfold & income columns\n",
    "    features = [f for f in df.columns if f not in (\"kfold\", \"income\")]\n",
    "    \n",
    "    # fill all NaN values with NONE\n",
    "    # note that all columns are converted to \"strings\"\n",
    "    # it doesn't matter because all are categories\n",
    "    for col in features:\n",
    "        # do not encode the numerical columns\n",
    "        if col not in num_cols:\n",
    "            df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
    "        \n",
    "    # now its time to label encode the features\n",
    "    for col in features:\n",
    "        if col not in num_cols:\n",
    "\n",
    "            # initialize LabelEncoder for each column\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "\n",
    "            # fit label encoder on all data\n",
    "            lbl.fit(df[col])\n",
    "\n",
    "            # transform all the data\n",
    "            df.loc[:, col] = lbl.transform(df[col])\n",
    "        \n",
    "    # get training data using folds\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    \n",
    "    # get validation data using folds\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "      \n",
    "    # get training data\n",
    "    x_train = df_train[features].values\n",
    "    \n",
    "    # get validation data\n",
    "    x_valid = df_valid[features].values\n",
    "    \n",
    "    # initialize XGBoost model\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # fit model on training data \n",
    "    model.fit(x_train, df_train.income.values)\n",
    "    \n",
    "    # prediction on validation data\n",
    "    # we need probability values as we are calculating AUC\n",
    "    # we will use the probability of 1s\n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1]\n",
    "    \n",
    "    # get roc_auc score\n",
    "    auc = metrics.roc_auc_score(df_valid.income.values, valid_preds)\n",
    "        \n",
    "    # print auc\n",
    "    print(f\"Fold = {fold}, AUC = {auc}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for fold_ in range(5):\n",
    "        run(fold_)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We get a little improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:17:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fold = 0, AUC = 0.9237966821775458\n",
      "[23:17:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fold = 1, AUC = 0.9263750175434251\n",
      "[23:17:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fold = 2, AUC = 0.9232032049773793\n",
      "[23:17:39] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fold = 3, AUC = 0.9226309152301697\n",
      "[23:17:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fold = 4, AUC = 0.9182498962873985\n"
     ]
    }
   ],
   "source": [
    "# Increase max_depth to 7 \n",
    "# lbl_xgb_num_feat.py\n",
    "import itertools\n",
    "import pandas as pd \n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def feature_engineering(df, cat_cols):\n",
    "    \"\"\"\n",
    "    This function is used for feature engineering\n",
    "    :param df: the pandas dataframe with train/test data\n",
    "    :param cat-cols: list if categorical columns\n",
    "    :return: dataframe with new features\n",
    "    \"\"\"\n",
    "    # this will create all 2-combinations of values in this list\n",
    "    # for example:\n",
    "    # list(itertools.combinations([1,2,3], 2)) will return\n",
    "    # [(1,2), (1,3), (2,3)]\n",
    "    combi = list(itertools.combinations(cat_cols, 2))\n",
    "    for c1, c2 in combi:\n",
    "        df.loc[:, c1 + \"_\" + c2] = df[c1].astype(str) + \"_\" + df[c2].astype(str)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def run(fold):\n",
    "    # load the full training data with folds\n",
    "    df = pd.read_csv(\"adult_folds.csv\")\n",
    "    \n",
    "    # list of numerical columns\n",
    "    num_cols = [\n",
    "        \"fnlwgt\",\n",
    "        \"age\",\n",
    "        \"capital.gain\",\n",
    "        \"capital.loss\",\n",
    "        \"hours.per.week\"\n",
    "    ]\n",
    "    \n",
    "    # map targets to 0s and 1s\n",
    "    target_mapping = {\n",
    "        \"<=50K\": 0,\n",
    "        \">50K\": 1\n",
    "    }\n",
    "    df.loc[:, \"income\"] = df.income.map(target_mapping)\n",
    "    \n",
    "    # list of categorical columns for feature engineering \n",
    "    cat_cols = [c for c in df.columns if c not in num_cols and c not in (\"kfold\", \"income\")]\n",
    "    \n",
    "    # add new features\n",
    "    df = feature_engineering(df, cat_cols)\n",
    "    \n",
    "    # all columns are features except kfold & income columns\n",
    "    features = [f for f in df.columns if f not in (\"kfold\", \"income\")]\n",
    "    \n",
    "    # fill all NaN values with NONE\n",
    "    # note that all columns are converted to \"strings\"\n",
    "    # it doesn't matter because all are categories\n",
    "    for col in features:\n",
    "        # do not encode the numerical columns\n",
    "        if col not in num_cols:\n",
    "            df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
    "        \n",
    "    # now its time to label encode the features\n",
    "    for col in features:\n",
    "        if col not in num_cols:\n",
    "\n",
    "            # initialize LabelEncoder for each column\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "\n",
    "            # fit label encoder on all data\n",
    "            lbl.fit(df[col])\n",
    "\n",
    "            # transform all the data\n",
    "            df.loc[:, col] = lbl.transform(df[col])\n",
    "        \n",
    "    # get training data using folds\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    \n",
    "    # get validation data using folds\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "      \n",
    "    # get training data\n",
    "    x_train = df_train[features].values\n",
    "    \n",
    "    # get validation data\n",
    "    x_valid = df_valid[features].values\n",
    "    \n",
    "    # initialize XGBoost model\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_jobs=-1,\n",
    "        max_depth=7\n",
    "    )\n",
    "    \n",
    "    # fit model on training data \n",
    "    model.fit(x_train, df_train.income.values)\n",
    "    \n",
    "    # prediction on validation data\n",
    "    # we need probability values as we are calculating AUC\n",
    "    # we will use the probability of 1s\n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1]\n",
    "    \n",
    "    # get roc_auc score\n",
    "    auc = metrics.roc_auc_score(df_valid.income.values, valid_preds)\n",
    "        \n",
    "    # print auc\n",
    "    print(f\"Fold = {fold}, AUC = {auc}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for fold_ in range(5):\n",
    "        run(fold_)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have improved the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Encoding + XGBoost\n",
    "\n",
    "### `target_encoding.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[00:08:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fold = 0, AUC = 0.9256790779483561\n",
      "[00:08:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fold = 1, AUC = 0.9260629767683775\n",
      "[00:08:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fold = 2, AUC = 0.9250695803199921\n",
      "[00:08:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fold = 3, AUC = 0.9229308304025493\n",
      "[00:08:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Fold = 4, AUC = 0.9196802659170464\n"
     ]
    }
   ],
   "source": [
    "# target_encoding.py\n",
    "\n",
    "import copy\n",
    "import pandas as pd \n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "import xgboost as xgb \n",
    "\n",
    "def mean_target_encoding(data):\n",
    "    \n",
    "    # make a copy of dataframe\n",
    "    df = copy.deepcopy(data)\n",
    "    \n",
    "    # list of numerical columns\n",
    "    num_cols = [\n",
    "        \"fnlwgt\",\n",
    "        \"age\",\n",
    "        \"capital.gain\",\n",
    "        \"capital.loss\",\n",
    "        \"hours.per.week\"\n",
    "    ]\n",
    "    \n",
    "    # map targets to 0s and 1s\n",
    "    target_mapping = {\n",
    "        \"<=50K\": 0,\n",
    "        \">50K\": 1\n",
    "    }\n",
    "    df.loc[:, \"income\"] = df.income.map(target_mapping)\n",
    "    \n",
    "    # all columns are features except kfold & income columns\n",
    "    features = [f for f in df.columns if f not in (\"kfold\", \"income\") and f not in num_cols]\n",
    "    \n",
    "    # fill all NaN values with NONE\n",
    "    # note that all columns are converted to \"strings\"\n",
    "    # it doesn't matter because all are categories\n",
    "    for col in features:\n",
    "        # do not encode the numerical columns\n",
    "        if col not in num_cols:\n",
    "            df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
    "        \n",
    "    # now its time to label encode the features\n",
    "    for col in features:\n",
    "        if col not in num_cols:\n",
    "\n",
    "            # initialize LabelEncoder for each column\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "\n",
    "            # fit label encoder on all data\n",
    "            lbl.fit(df[col])\n",
    "\n",
    "            # transform all the data\n",
    "            df.loc[:, col] = lbl.transform(df[col])\n",
    "            \n",
    "    # a list to store 5 validation features\n",
    "    encoded_dfs = []\n",
    "    \n",
    "    # go over all folds\n",
    "    for fold in range(5):\n",
    "        # fetch training and validation data\n",
    "        df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "        df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "        \n",
    "        # for all feature columns i.e. categorical columns\n",
    "        for column in features:\n",
    "            # create dict of category: mean target\n",
    "            mapping_dict = dict(\n",
    "            df_train.groupby(column)[\"income\"].mean()\n",
    "            )\n",
    "            # column_enc is the new column we have with mean encoding\n",
    "            df_valid.loc[:, column + \"_enc\"] = df_valid[column].map(mapping_dict)\n",
    "        # append to our list of encoded validation dataframes\n",
    "        encoded_dfs.append(df_valid)\n",
    "    # create full data frame again and return\n",
    "    encoded_df = pd.concat(encoded_dfs, axis=0)\n",
    "    return encoded_df\n",
    "\n",
    "def run(df, fold):\n",
    "    # note that folds are same as before        \n",
    "    # get training data using folds\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    \n",
    "    # get validation data using folds\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "    \n",
    "    # all columns ar features except income and kfold columns\n",
    "    features = [f for f in df.columns if f not in (\"kfold\", \"income\")]\n",
    "    \n",
    "    # scale training data\n",
    "    x_train = df_train[features].values\n",
    "    \n",
    "    # scale validation data\n",
    "    x_valid = df_valid[features].values\n",
    "    \n",
    "    # initialize XGBoost model\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_jobs=-1,\n",
    "        max_depth=7\n",
    "    )\n",
    "    \n",
    "    # fit model on training data \n",
    "    model.fit(x_train, df_train.income.values)\n",
    "    \n",
    "    # prediction on validation data\n",
    "    # we need probability values as we are calculating AUC\n",
    "    # we will use the probability of 1s\n",
    "    valid_preds = model.predict_proba(x_valid)[:, 1]\n",
    "    \n",
    "    # get roc_auc score\n",
    "    auc = metrics.roc_auc_score(df_valid.income.values, valid_preds)\n",
    "        \n",
    "    # print auc\n",
    "    print(f\"Fold = {fold}, AUC = {auc}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # read data\n",
    "    df = pd.read_csv(\"adult_folds.csv\")\n",
    "    \n",
    "    # create mean target encoded categories and munge data\n",
    "    df = mean_target_encoding(df)\n",
    "    \n",
    "    # run training and validation for 5 folds\n",
    "    for fold_ in range(5):\n",
    "        run(df, fold_)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have improved again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity Embeddings\n",
    "\n",
    "### `entity_embeddings.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "469/469 [==============================] - 45s 75ms/step - loss: 0.4730 - val_loss: 0.4152\n",
      "Epoch 2/3\n",
      "469/469 [==============================] - 38s 81ms/step - loss: 0.4085 - val_loss: 0.3973\n",
      "Epoch 3/3\n",
      "469/469 [==============================] - 41s 87ms/step - loss: 0.4025 - val_loss: 0.3973\n",
      "0.7869503440759662\n",
      "Epoch 1/3\n",
      "469/469 [==============================] - 46s 83ms/step - loss: 0.4730 - val_loss: 0.4140\n",
      "Epoch 2/3\n",
      "469/469 [==============================] - 42s 89ms/step - loss: 0.4082 - val_loss: 0.4000\n",
      "Epoch 3/3\n",
      "469/469 [==============================] - 39s 83ms/step - loss: 0.4023 - val_loss: 0.3993\n",
      "0.7841058954388263\n",
      "Epoch 1/3\n",
      "469/469 [==============================] - 59s 87ms/step - loss: 0.4744 - val_loss: 0.4183\n",
      "Epoch 2/3\n",
      "469/469 [==============================] - 38s 81ms/step - loss: 0.4083 - val_loss: 0.3989\n",
      "Epoch 3/3\n",
      "469/469 [==============================] - 40s 85ms/step - loss: 0.4023 - val_loss: 0.3981\n",
      "0.7856915935750822\n",
      "Epoch 1/3\n",
      "469/469 [==============================] - 75s 132ms/step - loss: 0.4739 - val_loss: 0.4184\n",
      "Epoch 2/3\n",
      "469/469 [==============================] - 50s 107ms/step - loss: 0.4085 - val_loss: 0.3991\n",
      "Epoch 3/3\n",
      "469/469 [==============================] - 52s 112ms/step - loss: 0.4021 - val_loss: 0.3987\n",
      "0.7840908747841048\n",
      "Epoch 1/3\n",
      "469/469 [==============================] - 72s 130ms/step - loss: 0.4743 - val_loss: 0.4174\n",
      "Epoch 2/3\n",
      "469/469 [==============================] - 54s 116ms/step - loss: 0.4085 - val_loss: 0.4001\n",
      "Epoch 3/3\n",
      "469/469 [==============================] - 52s 111ms/step - loss: 0.4027 - val_loss: 0.3997\n",
      "0.7842464506597762\n"
     ]
    }
   ],
   "source": [
    "# entity_embeddings.py\n",
    "import os \n",
    "import gc \n",
    "import joblib\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "from sklearn import metrics, preprocessing\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "def create_model(data, catcols):\n",
    "    \"\"\"\n",
    "    This function returns a compiled tf.keras model for entity embeddings\n",
    "    :param data: this is pandas dataframe\n",
    "    :param catcols: list of categorical column names\n",
    "    :return: compiled tf.keras model\n",
    "    \"\"\"\n",
    "    \n",
    "    # init list of inputs for embeddings\n",
    "    inputs = []\n",
    "    \n",
    "    # init list of outputs for embeddings\n",
    "    outputs = []\n",
    "    \n",
    "    # loop over all categorical columns\n",
    "    for c in catcols:\n",
    "        # find the number of unique values in the column\n",
    "        num_unique_values = int(data[c].nunique())\n",
    "        \n",
    "        # simple dimension of embedding calculator\n",
    "        # min size is half of the number of unique values\n",
    "        # max size is 50. max size depends on the number of unique categories too. 50 is quite sufficient most of the times\n",
    "        # but if you have millions of unique values, you might need a larger dimension\n",
    "        embed_dim = int(min(np.ceil((num_unique_values)/2), 50))\n",
    "        \n",
    "        # simple keras input layer with size 1\n",
    "        inp = layers.Input(shape=(1,))\n",
    "        \n",
    "        # add embedding layer to raw input\n",
    "        # embedding size is always 1 more than unique values in input\n",
    "        out = layers.Embedding(\n",
    "            num_unique_values + 1, embed_dim, name=c\n",
    "        )(inp)\n",
    "        \n",
    "        # 1D spatial dropout is the standard for embedding layers you can use it in NLP tasks too\n",
    "        out = layers.SpatialDropout1D(0.3)(out)\n",
    "        \n",
    "        # reshape the input to the dimension of embedding this becomes our output layer for current feature\n",
    "        out = layers.Reshape(target_shape=(embed_dim, ))(out)\n",
    "        \n",
    "        # add input to input list\n",
    "        inputs.append(inp)\n",
    "        \n",
    "        # add output to output list\n",
    "        outputs.append(out)\n",
    "        \n",
    "    # concatenate all output layers\n",
    "    x = layers.Concatenate()(outputs)\n",
    "        \n",
    "    # add a batchnorm layer.\n",
    "    # from here, everything is up to you\n",
    "    # you can try different architectures\n",
    "    # this is the architecture I like to use\n",
    "    # if you have numerical features, you should add them or in concatenate layer\n",
    "    x = layers.BatchNormalization()(x)\n",
    "        \n",
    "    # a bunch of dense layers with dropout.\n",
    "    # start with 1 or two layers only\n",
    "    x = layers.Dense(300, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Dense(300, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # using softmax and treating it as a two class problem \n",
    "    # you can also use sigmoid, then you need to use only one output class\n",
    "    y = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "    # create final model\n",
    "    model = Model(inputs=inputs, outputs=y)\n",
    "\n",
    "    # compile the model\n",
    "    # we use adam and binary cross entropy\n",
    "    # feel free to use something else and see how model behaves\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return model \n",
    "\n",
    "def run(fold):\n",
    "    # load the full training data with folds\n",
    "    df = pd.read_csv(\"cat_train_folds.csv\")\n",
    "    \n",
    "    # all columns are features except id, target and kfold columns\n",
    "    features = [f for f in df.columns if f not in (\"id\", \"target\", \"kfold\")]\n",
    "    \n",
    "    # fill all NaN values with NONE\n",
    "    # note that all columns are converted to \"strings\"\n",
    "    # it doesn't matter because all are categories\n",
    "    for col in features:\n",
    "        df.loc[:, col] = df[col].astype(str).fillna(\"NONE\")\n",
    "        \n",
    "    # encode all features with label encoder individually\n",
    "    # in a live setting you need to save all label encoders\n",
    "    for feat in features:\n",
    "        lbl_enc = preprocessing.LabelEncoder()\n",
    "        df.loc[:, feat] = lbl_enc.fit_transform(df[feat].values)\n",
    "        \n",
    "        \n",
    "    # get training data using folds\n",
    "    df_train = df[df.kfold != fold].reset_index(drop=True)\n",
    "    \n",
    "    # get validation data using folds\n",
    "    df_valid = df[df.kfold == fold].reset_index(drop=True)\n",
    "      \n",
    "    # create tf.keras model\n",
    "    model = create_model(df, features)\n",
    "    \n",
    "    # our features are lists of lists\n",
    "    xtrain = [\n",
    "        df_train[features].values[:, k] for k in range(len(features))\n",
    "    ]\n",
    "    xvalid = [\n",
    "        df_valid[features].values[:, k] for k in range(len(features))\n",
    "    ]\n",
    "    \n",
    "    # fetch target columns\n",
    "    ytrain = df_train.target.values\n",
    "    yvalid = df_valid.target.values\n",
    "    \n",
    "    # convert target columns to categories \n",
    "    # this is just binarization\n",
    "    ytrain_cat = utils.to_categorical(ytrain)\n",
    "    yvalid_cat = utils.to_categorical(yvalid)\n",
    "    \n",
    "    # fit the model\n",
    "    model.fit(xtrain,\n",
    "              ytrain_cat,\n",
    "              validation_data=(xvalid, yvalid_cat),\n",
    "              verbose=1,\n",
    "              batch_size=1024,\n",
    "              epochs=3\n",
    "             )\n",
    "    \n",
    "    # generate validation predictions\n",
    "    valid_preds = model.predict(xvalid)[:, 1]\n",
    "    \n",
    "    # print roc auc score\n",
    "    print(metrics.roc_auc_score(yvalid, valid_preds))\n",
    "    \n",
    "    # clear session to free up some GPU memory\n",
    "    K.clear_session()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run(0)\n",
    "    run(1)\n",
    "    run(2)\n",
    "    run(3)\n",
    "    run(4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
